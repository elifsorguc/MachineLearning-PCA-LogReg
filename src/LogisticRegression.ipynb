{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function for one-hot encoding\n",
    "def one_hot_encoding(label_data):\n",
    "    num_samples = label_data.shape[0]\n",
    "    num_classes = 10  # Assuming 10 classes (0-9)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes), dtype='int')\n",
    "    encoded_labels[np.arange(num_samples), label_data] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "# Function to read pixel data\n",
    "def read_pixels(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        raw_data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    flattened_pixels = raw_data.reshape(-1, 784).astype('float32')\n",
    "    return flattened_pixels / 255.0\n",
    "\n",
    "\n",
    "# Function to read label data\n",
    "def read_labels(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        raw_labels = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "    return one_hot_encoding(raw_labels)\n",
    "\n",
    "\n",
    "# Function to load the MNIST dataset\n",
    "def load_mnist_data(path):\n",
    "    X_train = read_pixels(os.path.join(path, \"train-images-idx3-ubyte\"))\n",
    "    y_train = read_labels(os.path.join(path, \"train-labels-idx1-ubyte\"))\n",
    "    X_test = read_pixels(os.path.join(path, \"t10k-images-idx3-ubyte\"))\n",
    "    y_test = read_labels(os.path.join(path, \"t10k-labels-idx1-ubyte\"))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=1e-3, reg_lambda=1e-4, epochs=50, batch_size=200, input_dim=784, output_dim=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = np.random.normal(0, 1, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Numerical stability\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))  # Avoid log(0)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        num_samples = X_train.shape[0]\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                X_batch = X_train[start:start + self.batch_size]\n",
    "                y_batch = y_train[start:start + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                logits = np.dot(X_batch, self.weights) + self.biases\n",
    "                predictions = self.softmax(logits)\n",
    "\n",
    "                # Compute gradients\n",
    "                error = predictions - y_batch\n",
    "                grad_weights = np.dot(X_batch.T, error) / self.batch_size + self.reg_lambda * self.weights\n",
    "                grad_biases = np.sum(error, axis=0, keepdims=True) / self.batch_size\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights -= self.learning_rate * grad_weights\n",
    "                self.biases -= self.learning_rate * grad_biases\n",
    "\n",
    "            # Print validation accuracy for each epoch\n",
    "            val_accuracy = self.evaluate(X_val, y_val)\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.weights) + self.biases\n",
    "        return np.argmax(self.softmax(logits), axis=1)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        return np.mean(y_true_labels == y_pred)\n",
    "\n",
    "\n",
    "# Display sample images\n",
    "def display_sample_images(X_test, y_test, num_samples=3):\n",
    "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        image = X_test[idx].reshape(28, 28)\n",
    "        label = np.argmax(y_test[idx])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"/Users/elifsorguc/Desktop/Bilkent/ML/MachineLearning-PCA-LogReg/data/mnist\"\n",
    "\n",
    "    # Load dataset\n",
    "    X_train, y_train, X_test, y_test = load_mnist_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# sample\n",
    "if __name__ == \"__main__\":\n",
    "    # Display sample images\n",
    "    display_sample_images(X_test, y_test, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "if __name__ == \"__main__\":\n",
    "    # Split validation set from training data\n",
    "    X_val, y_val = X_train[:10000], y_train[:10000]\n",
    "    X_train, y_train = X_train[10000:], y_train[10000:]\n",
    "\n",
    "    # Train model\n",
    "    model = LogisticRegression()\n",
    "    model.train(X_train, y_train, X_val, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_true = np.argmax(y_test, axis=1)\n",
    "    cm = np.zeros((10, 10), dtype=int)\n",
    "    for true, pred in zip(y_test_true, y_test_pred):\n",
    "        cm[true, pred] += 1\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    class_names = [\n",
    "        \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "        \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "    ]\n",
    "    plot_confusion_matrix(cm, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
