{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function for one-hot encoding\n",
    "def one_hot_encoding(label_data):\n",
    "    num_samples = label_data.shape[0]\n",
    "    num_classes = 10  # Assuming 10 classes (0-9)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes), dtype='int')\n",
    "    encoded_labels[np.arange(num_samples), label_data] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "# Function to read pixel data\n",
    "def read_pixels(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        raw_data = np.frombuffer(f.read(), dtype=np.uint8, offset=16)\n",
    "    return raw_data.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "\n",
    "# Function to read label data\n",
    "def read_labels(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        raw_labels = np.frombuffer(f.read(), dtype=np.uint8, offset=8)\n",
    "    return one_hot_encoding(raw_labels)\n",
    "\n",
    "\n",
    "# Function to load the MNIST dataset\n",
    "def load_mnist_data(path):\n",
    "    X_train = read_pixels(os.path.join(path, \"train-images-idx3-ubyte\"))\n",
    "    y_train = read_labels(os.path.join(path, \"train-labels-idx1-ubyte\"))\n",
    "    X_test = read_pixels(os.path.join(path, \"t10k-images-idx3-ubyte\"))\n",
    "    y_test = read_labels(os.path.join(path, \"t10k-labels-idx1-ubyte\"))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=1e-3, reg_lambda=1e-4, epochs=50, batch_size=200, input_dim=784, output_dim=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = np.random.normal(0, 1, (input_dim, output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        \"\"\"Computes softmax probabilities for the given logits.\"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # For numerical stability\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))  # Avoid log(0)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        num_samples = X_train.shape[0]\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                X_batch = X_train[start:start + self.batch_size]\n",
    "                y_batch = y_train[start:start + self.batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                logits = np.dot(X_batch, self.weights) + self.biases\n",
    "                predictions = self.softmax(logits)\n",
    "\n",
    "                # Compute gradients\n",
    "                error = predictions - y_batch\n",
    "                grad_weights = np.dot(X_batch.T, error) / self.batch_size + self.reg_lambda * self.weights\n",
    "                grad_biases = np.sum(error, axis=0, keepdims=True) / self.batch_size\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights -= self.learning_rate * grad_weights\n",
    "                self.biases -= self.learning_rate * grad_biases\n",
    "\n",
    "            val_accuracy, _ = self.evaluate(X_val, y_val)\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.weights) + self.biases\n",
    "        return np.argmax(self.softmax(logits), axis=1)\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(y_true_labels == y_pred)\n",
    "        return accuracy, self.confusion_matrix(y_true_labels, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred):\n",
    "        cm = np.zeros((10, 10), dtype=int)\n",
    "        for t, p in zip(y_true, y_pred):\n",
    "            cm[t, p] += 1\n",
    "        return cm\n",
    "\n",
    "\n",
    "# Display sample images\n",
    "def display_sample_images(X_test, y_test, num_samples=3):\n",
    "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "    for i, idx in enumerate(indices):\n",
    "        image = X_test[idx].reshape(28, 28)\n",
    "        label = np.argmax(y_test[idx])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def run_hyperparameter_experiments():\n",
    "    \"\"\"\n",
    "    Run experiments for different batch sizes and generate a comparison graph.\n",
    "    \"\"\"\n",
    "    # Batch sizes to test\n",
    "    batch_sizes = [1, 64, 1000]  # Example batch sizes\n",
    "    epochs = 100  # Total number of epochs\n",
    "    batch_size_results = {}\n",
    "\n",
    "    # Run experiment for each batch size\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Running experiment for Batch Size: {batch_size}\")\n",
    "        \n",
    "        # Reinitialize the model for each batch size\n",
    "        model = LogisticRegression(batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        # Train the model and track validation accuracies\n",
    "        validation_accuracies = []\n",
    "        for epoch in range(epochs):\n",
    "            model.train(X_train, y_train, X_val, y_val)\n",
    "            val_accuracy, _ = model.evaluate(X_val, y_val)\n",
    "            validation_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Store results for plotting\n",
    "        batch_size_results[f\"Batch Size: {batch_size}\"] = validation_accuracies\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for batch_size, accuracies in batch_size_results.items():\n",
    "        plt.plot(range(1, epochs + 1), accuracies, label=f\"Batch Size: {batch_size}\")\n",
    "    plt.title(\"Validation Accuracy vs. Epochs for Different Batch Sizes\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset_path = \"/Users/elifsorguc/Desktop/Bilkent/ML/MachineLearning-PCA-LogReg/data/mnist\"\n",
    "    X_train, y_train, X_test, y_test = load_mnist_data(dataset_path)\n",
    "\n",
    "    # Split validation set from training data\n",
    "    X_val, y_val = X_train[:10000], y_train[:10000]\n",
    "    X_train, y_train = X_train[10000:], y_train[10000:]\n",
    "\n",
    "    # Display sample images\n",
    "    display_sample_images(X_test, y_test)\n",
    "\n",
    "    # Run hyperparameter experiments for batch sizes\n",
    "    run_hyperparameter_experiments()\n",
    "\n",
    "    # Generate confusion matrix for final model\n",
    "    model = LogisticRegression()\n",
    "    model.train(X_train, y_train, X_val, y_val)\n",
    "    _, cm = model.evaluate(X_test, y_test)\n",
    "    class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "    plot_confusion_matrix(cm, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
